{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lesson 1: Overview of A/B testing \n",
    "\n",
    "# First, we should determinate whether AB testing can be used. (difficult and important! )\n",
    "# Select baseline and time, AB testing need to be made in a short period and easy to gather data. For example, comparing new and old version of apps. control and experiment group are run at the same time. \n",
    "# Second, we make a hypothesis and select appropriate metric. For instance,clik through rate(CTR) for measuring usability or clik through probability(unique visitors who clik/unique visitors to page)\n",
    "# Thrid, hypothesis testing and figure out whether exsiting practical significance.(Important: Statistical significance bar is lower than practical significance bar). 2%(dmin) can be used for practical significance \n",
    "# which means we need to decide the expected value before we make testing(prior)\n",
    "\n",
    "# Lesson 2: Policy and ethics for experiments\n",
    "# 1. Every employee who might be involved in A/B test be educated about the ethics and the protection of the participants. Clearly there are other areas of ethics beyond what weâ€™ve covered that discuss integrity, competence, and responsibility, but those generally are broader than protecting participants of A/B tests (cite ACM code of ethics).\n",
    "# 2. All data, identified or not, be stored securely, with access limited to those who need it to complete their job. Access should be time limited. There should be clear policies of what data usages are acceptable and not acceptable. Moreover, all usage of the data should be logged and audited regularly for violations.\n",
    "# 3. You create a clear escalation path for how to handle cases where there is even possibly more than minimal risk or data sensitivity issues.\n",
    "\n",
    "# Lesson 3: choosing and charaterizing metric\n",
    "# Creating high level metric\n",
    "# Hard to collect data or take a long time\n",
    "# User experience research, focus groups, surveys\n",
    "# def 1: for each (time interval) cookies/ cookies\n",
    "# def 2: pageviews_results_clik within (time interval) /pageviews \n",
    "# def 3: num_cliks/num_pageviews clik-through-rate\n",
    "# week over week data or year over year data \n",
    "# sum, mean, median, 25th,75th,90th percentiles, probability, rate, ratio. mean has sensitivity and median has robustness. we have to choose the metric which is appropriately sensitive and robust\n",
    "# (Absolute or relative difference) The total downloads of game market could influence the difference. Countrty segmentation could cause the rule of total downloads of game market. \n",
    "# Rates distributes poisson. \n",
    "# AA test for determinating underlying variability. bootstrap method also is helpful. First, using bootstrap then deciding to use AA test.\n",
    "\n",
    "# Lesson 4: Designing an experiment\n",
    "# Data does not have independent can cause the analytical variability are higher than empirical variability.  \n",
    "# inter-user experiment and intra-user experiment\n",
    "# divertion of population. \n",
    "# Duration and when I run my experiment. \n",
    "# AA test ---> AB test ---> AA test \n",
    "\n",
    "# Lesson 5: Analyzing results\n",
    "# First, sanity check. Checking invariants in each group, choosing invariant metrics.\n",
    "# Sign test or wilcoxon rank sum test, null hypothesis is: identical  \n",
    "# FDR = control false discovery rate\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
