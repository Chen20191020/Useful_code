{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coding=utf8\n",
    "import pymysql\n",
    "from scipy import stats\n",
    "import requests,json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from datetime import datetime\n",
    "import random \n",
    "import re\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import math as m\n",
    "import statsmodels.formula.api\n",
    "\n",
    "# def get_data(sql, format='csv_header '):\n",
    "#     \"\"\"\n",
    "#     根据输入的SQL语句，返回数数科技查询结果\n",
    "#     :param sql: SQL语句\n",
    "#     :param format: 默认为'csv-header', 可选择的还有json\n",
    "#     :return: 返回DataFrame\n",
    "#     \"\"\"\n",
    "#     headers = {\n",
    "#         'Content-Type': 'application/x-www-form-urlencoded',\n",
    "#     }\n",
    "\n",
    "#     data = {\n",
    "#         'sql': sql,\n",
    "#         'format': format\n",
    "#     }\n",
    "#     url = 'http://101.32.192.62:8992/querySql?token=bjMIEMvVgCYH3ngtjTw1hngzuKpdqSkI1GoO1A0NkJQdubjrmHmYBGCH8HRE20tU'\n",
    "#     r = requests.post(url, headers=headers, data=data)\n",
    "#     data = StringIO(r.text)  # 将返回数据转化为StringIO对象\n",
    "\n",
    "#     json_head = json.loads(data.readline())  # 获取头数据，并转化为字典\n",
    "#     column_name = json_head['data']['headers']\n",
    "#     column_name = [re.findall('\\w.*', k)[0] for k in column_name]  # 去除字段名中的不规范字符\n",
    "\n",
    "#     # 提取行数据，并对格式进行转化\n",
    "#     result = [json.loads(line) for line in data.readlines()]\n",
    "\n",
    "#     # 生成DataFrame\n",
    "#     df = pd.DataFrame(result, columns=column_name).convert_dtypes()\n",
    "\n",
    "#     return df\n",
    "\n",
    "# # table name: ta.v_event_6\n",
    "# \"#event_name\", \"#account_id\", \"channel\",\"media_source\",\"#country_code\", \"app_version\"\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     sql = \"\"\"\n",
    "#     SELECT \"$part_event\",\"$part_date\",\"#user_id\",\"#event_name\",\"#event_time\",\"#account_id\",\"#distinct_id\",\"#server_time\",\"channel\",\"platform\",\"#zone_offset\",\"#city\",\"#ip\",\"carid\",\"#lib\",\"totalgamelength\",\"install_time\",\"#province\",\"ailevel\",\"deviceid\",\"version\",\"level\",\n",
    "#     \"carlevel\",\"state\",\"carjumptimers\",\"finalrewardamount\",\"systemarchitecture\",\"totalgametimesum\",\"totaltimes\",\"failfinishtype\",\"rwtype\",\"loadtime\",\"aggregatelevelfinishtimes\",\"required_enginelevel\",\"gaslevel\",\"required_gaslevel\",\"bonuslevel\",\"enginelevel\",\"finalreward_diamond\",\"aggregatediamondamount\",\"finalreward_coin\",\"aggregatecoinamount\",\"partsid\",\"tyreid\",\"racetype\",\"computeshadersupport\" FROM ta.v_event_6 WHERE \"$part_date\" = '2021-04-06' \n",
    "#     \"\"\"\n",
    "#     get_data(sql)\n",
    "\n",
    "\n",
    "def rename_table(name_list, df_data):\n",
    "    for (index, item) in enumerate(name_list):\n",
    "        df_data[item] = df_data[index]     \n",
    "    df_data = df_data.drop([i for i in range(len(name_list))], axis =1)\n",
    "    return df_data\n",
    "\n",
    "def list_to_frame(my_df):\n",
    "    df_aux = my_df.to_frame()\n",
    "    df_aux = df_aux.reset_index()\n",
    "    return df_aux\n",
    "\n",
    "def check_factor_level(factor, my_df):\n",
    "    drop = my_df.drop_duplicates(subset = [factor], keep = 'first')\n",
    "    return list(drop[factor])\n",
    "\n",
    "def str_to_date(obj):\n",
    "    return datetime.strptime(obj, '%m/%d/%Y')\n",
    "\n",
    "def date_to_str(obj):\n",
    "    return obj.strftime('%m/%d/%Y')\n",
    "\n",
    "def sort_date(date):\n",
    "    aux_date = list(map(str_to_date, date))\n",
    "    aux_date.sort()\n",
    "    final = list(map(date_to_str, aux_date))\n",
    "    return final\n",
    "\n",
    "def list_to_frame(my_df):\n",
    "    df_aux = my_df.to_frame()\n",
    "    df_aux = df_aux.reset_index()\n",
    "    return df_aux\n",
    "\n",
    "def cleaning(file_path, start_date, end_date):\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['count'] = 1\n",
    "    aux = data[data['#event_name'] == 'mpmf_ad_reward'].groupby(['#account_id', '$part_date', 'app_version'])['count'].sum()\n",
    "    df = list_to_frame(aux)\n",
    "    start_date = datetime.strptime(start_date,'%m/%d/%Y')\n",
    "    end_date = datetime.strptime(end_date,'%m/%d/%Y')\n",
    "    df['datetime'] = df['$part_date'].apply(lambda x:datetime.strptime(x,'%m/%d/%Y'))\n",
    "    df = df[(df['datetime'] >= start_date) & (df['datetime'] <= end_date) ]\n",
    "    cleaned_df = df.groupby(['app_version','#account_id']).sum().reset_index()\n",
    "    return cleaned_df\n",
    "\n",
    "def datelist(start_date, end_date):\n",
    "    # beginDate, endDate是形如‘20210409’的字符串或datetime格式\n",
    "    date_l=[datetime.strftime(x,'%Y/%m/%d') for x in list(pd.date_range(start=start_date, end=end_date))]\n",
    "    return date_l\n",
    "\n",
    "pd.set_option('display.max_rows', 2000)\n",
    "\n",
    "\n",
    "\n",
    "# Get data frame\n",
    "\n",
    "# file_path: \"20210408_024723_00382_teus3.csv\"\n",
    "# ins_date: \"2021/1/31\"\n",
    "# date: \"2021/1/31\"\n",
    "\n",
    "# date:要计算到的天数\n",
    "\n",
    "def data_clean_date(file_path, ins_date, date):  \n",
    "    \n",
    "    data = pd.read_csv(file_path)\n",
    "    data['count'] = 1\n",
    "    aux = data[data['#event_name'] == 'install'].groupby(['#account_id','$part_date', 'app_version'])['count'].sum()\n",
    "    df = list_to_frame(aux)\n",
    "    df['ins_date'] = df['$part_date']\n",
    "    df_ins = df.drop(['$part_date', 'count'], axis = 1)\n",
    "    df_date = df_ins[(df_ins['ins_date'] == ins_date)]\n",
    "    df_date['date'] = date\n",
    "    \n",
    "    re = data[data['#event_name'] == 'mpmf_ad_reward'].groupby(['#account_id', '$part_date', 'app_version'])['count'].sum()     \n",
    "    df_re = list_to_frame(re)\n",
    "    df_re['date'] = df_re['$part_date']\n",
    "    df_re = df_re.drop(['$part_date'], axis = 1)\n",
    "    \n",
    "    inte = data[data['#event_name'] == 'mpmf_ad_Inter'].groupby(['#account_id', '$part_date', 'app_version'])['count'].sum()     \n",
    "    df_in = list_to_frame(inte)\n",
    "    df_in['date'] = df_in['$part_date']\n",
    "    df_in = df_in.drop(['$part_date'], axis = 1)\n",
    "    \n",
    "    df_redate = pd.merge(df_date, df_re, how = 'left', on = ['#account_id', 'date', 'app_version'])\n",
    "    df_redate['count'] = df_redate['count'].where(df_redate['count'].notnull(), 0)\n",
    "    df_redate['reward'] = df_redate['count']\n",
    "    df_redate = df_redate.drop(['count'], axis = 1)\n",
    "    \n",
    "    df_indate = pd.merge(df_date, df_in, how = 'left', on = ['#account_id', 'date', 'app_version'])\n",
    "    df_indate['count'] = df_indate['count'].where(df_indate['count'].notnull(), 0)\n",
    "    df_indate['inter'] = df_indate['count']\n",
    "    df_indate = df_indate.drop(['count'], axis = 1)\n",
    "    \n",
    "    df_final = pd.merge(df_redate, df_indate, how = 'left', on = ['#account_id', 'app_version','ins_date','date'])\n",
    "    df_final['total'] = df_final['reward'] + df_final['inter']\n",
    " \n",
    "    return df_final \n",
    "\n",
    "\n",
    "def specific_merge(df_date1, df_date2):\n",
    "    df_date1 = df_date1.drop(['app_version', 'ins_date'], axis = 1)\n",
    "    df_acudate = pd.merge(df_date1, df_date2, how = 'left', on = ['#account_id'])\n",
    "    df_acudate['reward'] = df_acudate['reward_x'] + df_acudate['reward_y']\n",
    "    df_acudate['inter'] = df_acudate['inter_x'] + df_acudate['inter_y']\n",
    "    df_acudate['total'] = df_acudate['total_x'] + df_acudate['total_y']\n",
    "    df_acudate['date'] = df_acudate['date_x']\n",
    "    df_acudate = df_acudate.drop(['reward_x', 'reward_y', 'inter_x', 'inter_y','total_x', 'total_y','date_x', 'date_y'], axis = 1)\n",
    "    return df_acudate\n",
    "\n",
    "\n",
    "def data_clean_acumulate_date(file_path, ins_date, date_interval):\n",
    "    \n",
    "    final = data_clean_date(file_path, ins_date, date_interval[0])\n",
    "    if len(date_interval) > 1: \n",
    "        for i in date_interval[1:]:\n",
    "            df_aux = data_clean_date(file_path, ins_date, i)\n",
    "            final = specific_merge(final, df_aux)\n",
    "        final['start_date'] = date_interval[0]\n",
    "        final['end_date'] = date_interval[len(date_interval)-1]\n",
    "        final = final.drop(['date'], axis = 1)\n",
    "        return final\n",
    "    else: \n",
    "        final['start_date'] = date_interval[0]\n",
    "        final['end_date'] = date_interval[len(date_interval)-1]\n",
    "        final = final.drop(['date'], axis = 1)\n",
    "        return final \n",
    "\n",
    "\n",
    "\n",
    "def data_cleaning_final(path, ins_date, date_interval):\n",
    "    df_aux = pd.DataFrame()\n",
    "    for i in range(len(date_interval)):\n",
    "        df = data_clean_acumulate_date(path, ins_date, date_interval[0:(i + 1)])\n",
    "        df_aux = pd.concat([df_aux, df])\n",
    "    df_aux = df_aux.reset_index(drop = True)\n",
    "    return df_aux\n",
    "\n",
    "\n",
    "def trimmed_sample_left(colname, df, trim_level):\n",
    "    final = pd.DataFrame()\n",
    "    date_in = list(df.drop_duplicates(subset = ['end_date'], keep = 'first')['end_date'])\n",
    "    for i in date_in:\n",
    "        aux_1 = df[(df['end_date'] == i) & (df['app_version'] == '1.1.3')].sort_values(by =[colname], ascending=True)\n",
    "        aux_2 = df[(df['end_date'] == i) & (df['app_version'] == '1.1.4')].sort_values(by =[colname], ascending=True)\n",
    "        aux_1 = aux_1.iloc[0:(m.ceil(aux_1.shape[0]*trim_level))]\n",
    "        aux_2 = aux_2.iloc[0:(m.ceil(aux_1.shape[0]*trim_level))]\n",
    "        aux_3 = pd.concat([aux_1, aux_2])\n",
    "        final = pd.concat([aux_3, final])\n",
    "    final = final.reset_index(drop = True)\n",
    "    return final \n",
    "\n",
    "\n",
    "# helpe = trimmed_sample('reward',df, 0.1)\n",
    "def trimmed_sample_right(colname, df, trim_level):\n",
    "    final = pd.DataFrame()\n",
    "    date_in = list(df.drop_duplicates(subset = ['end_date'], keep = 'first')['end_date'])\n",
    "    for i in date_in:\n",
    "        aux_1 = df[(df['end_date'] == i) & (df['app_version'] == '1.1.3')].sort_values(by =[colname], ascending=True)\n",
    "        aux_2 = df[(df['end_date'] == i) & (df['app_version'] == '1.1.4')].sort_values(by =[colname], ascending=True)\n",
    "        aux_1 = aux_1.iloc[(m.ceil(aux_1.shape[0]*(1-trim_level)) + 1):]\n",
    "        aux_2 = aux_2.iloc[(m.ceil(aux_2.shape[0]*(1-trim_level)) + 1):]\n",
    "        aux_3 = pd.concat([aux_1, aux_2])\n",
    "        final = pd.concat([aux_3, final])\n",
    "    final = final.reset_index(drop = True)\n",
    "    return final \n",
    "\n",
    "#helpe = trimmed_sample_left_right('reward',df, 0.1)\n",
    "def trimmed_sample_left_right(colname, df, trim_level):\n",
    "    final = pd.DataFrame()\n",
    "    date_in = list(df.drop_duplicates(subset = ['end_date'], keep = 'first')['end_date'])\n",
    "    for i in date_in:\n",
    "        aux_1 = df[(df['end_date'] == i) & (df['app_version'] == '1.1.3')].sort_values(by =[colname], ascending=True)\n",
    "        aux_2 = df[(df['end_date'] == i) & (df['app_version'] == '1.1.4')].sort_values(by =[colname], ascending=True)\n",
    "        aux_3 = aux_1.iloc[(m.ceil(aux_1.shape[0]*(1-trim_level)) + 1):]\n",
    "        aux_4 = aux_2.iloc[(m.ceil(aux_2.shape[0]*(1-trim_level)) + 1):]\n",
    "        aux_5 = aux_1.iloc[0:(m.ceil(aux_1.shape[0]*trim_level))]\n",
    "        aux_6 = aux_2.iloc[0:(m.ceil(aux_2.shape[0]*trim_level))]\n",
    "        aux_7 = pd.concat([aux_3, aux_4])\n",
    "        aux_8 = pd.concat([aux_5, aux_6])\n",
    "        aux_9 = pd.concat([aux_7, aux_8])\n",
    "        final = pd.concat([aux_9, final])\n",
    "    final = final.reset_index(drop = True)\n",
    "    return final \n",
    "\n",
    "\n",
    "def winsorized_sample(colname, df, trim_level):\n",
    "    final = pd.DataFrame()\n",
    "    date_in = list(df.drop_duplicates(subset = ['end_date'], keep = 'first')['end_date'])\n",
    "    for i in date_in:\n",
    "        aux_1 = df[(df['end_date'] == i) & (df['app_version'] == '1.1.3')].sort_values(by =[colname], ascending=True)\n",
    "        aux_2 = df[(df['end_date'] == i) & (df['app_version'] == '1.1.4')].sort_values(by =[colname], ascending=True)\n",
    "        aux_3 = aux_1.iloc[(m.ceil(aux_1.shape[0]*trim_level)):(m.ceil(aux_1.shape[0]*(1-trim_level)))]\n",
    "        aux_4 = aux_2.iloc[(m.ceil(aux_1.shape[0]*trim_level)):(m.ceil(aux_1.shape[0]*(1-trim_level)))]\n",
    "        aux_5 = pd.concat([aux_3, aux_4])\n",
    "        final = pd.concat([aux_5, final])\n",
    "    final = final.reset_index(drop = True)\n",
    "    return final \n",
    "\n",
    "\n",
    "\n",
    "# def get_csv():\n",
    "#     date_interval = datelist('20210131', '20210210')\n",
    "#     for i in range(len(date_interval)):\n",
    "#     df = data_clean_acumulate_date('20210408_024723_00382_teus3.csv', '2021/1/31', date_interval[0:(i + 1)])\n",
    "#     d0.to_csv(\"113114%s.csv\"%date_interval[i])\n",
    "\n",
    "\n",
    "# Experiment_design 数据处理\n",
    "# WOR_data = pd.read_csv('mpmf_mean(ad_reward)_20210308-20210314.csv')\n",
    "# WOR = for_experiment_design(WOR_data, 2)\n",
    "# WOR.to_csv('mpmf_人均_ad_reward_20210308-20210314.csv')\n",
    "\n",
    "# date_list = ['1/31/2021', '2/1/2021', '2/2/2021', '2/3/2021', '2/4/2021', '2/5/2021']\n",
    "\n",
    "# for i in date_list:\n",
    "#     obj_data = cleaning('20210331_113424_01352_teus3.csv', '1/31/2021', i)\n",
    "#     obj_data['count_num'] = 1\n",
    "#     print(obj_data.groupby(['app_version']).sum())\n",
    "\n",
    "# def count(data, num):\n",
    "#     count = 0 \n",
    "#     for i in data:\n",
    "#         if i > num:\n",
    "#             count = count + 1\n",
    "#     return print(num, count)\n",
    "\n",
    "# for i in range(1, 200):\n",
    "#     count(ver113, i)\n",
    "\n",
    "# mean113, mean114 = np.mean(d0[d0['app_version'] == '1.1.3']['count'].tolist()), np.mean(d0[d0['app_version'] == '1.1.4']['count'].tolist())\n",
    "# var113, var114 = np.var(d0[d0['app_version'] == '1.1.3']['count'].tolist()), np.var(d0[d0['app_version'] == '1.1.4']['count'].tolist())\n",
    "# qver113, qver114 = len(ver113)/(len(ver113) - 1) * var113, len(ver114)/(len(ver114) - 1) * var114\n",
    "# len(ver113), len(ver114),mean113, mean114, qver113, qver114\n",
    "\n",
    "# ver113 = d0[d0['app_version'] == '1.1.3']['count']\n",
    "# ver114 = d0[d0['app_version'] == '1.1.4']['count']\n",
    "# opt = dict(bins=30, histtype='stepfilled', edgecolor='none')\n",
    "# plt.axis([0, 200, 0, 2000])\n",
    "# plt.hist(ver113.to_numpy(), **opt)\n",
    "# plt.hist(ver114.to_numpy(), **opt)\n",
    "\n",
    "# pip install statsmodels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WOR_data = pd.read_csv('20210331_113424_01352_teus3.csv')\n",
    "# WOR_data = pd.read_csv('20210331_065708_00759_teus3.csv')\n",
    "# WOR_data.shape, WOR_data.columns\n",
    "# d0 = cleaning('20210331_113424_01352_teus3.csv', '1/31/2021', '1/31/2021')\n",
    "# d0.to_csv(\"113114.csv\")\n",
    "# obj_data[obj_data['app_version'] == '1.1.3']\n",
    "# v = ver113.to_numpy()\n",
    "# (v - np.mean(v))/np.var(v)**(1/2)\n",
    "# (1 - np.mean(v))/np.var(v)**(1/2)\n",
    "# (2 - np.mean(v))/np.var(v)**(1/2)\n",
    "\n",
    "# df[df['count'] > 1]\n",
    "# pd.set_option('display.max_columns', None)\n",
    "# data[(data['#account_id'] == '1612288316822-6842426706215486083') & (data['#event_name'] == 'install')]\n",
    "# data[(data['#account_id'] == '1612288316822-6842426706215486083')]\n",
    "\n",
    "# date_interval = ['2021/1/31', '2021/2/1','2021/2/2','2021/2/3','2021/2/4','2021/2/5','2021/2/6','2021/2/7','2021/2/8','2021/2/9','2021/2/10']\n",
    "# df.to_csv(\"113114.csv\")\n",
    "# final = pd.DataFrame()\n",
    "# date_interval = ['4/7/2021', '4/8/2021', '4/9/2021', '4/10/2021', '4/11/2021','4/12/2021','4/13/2021']\n",
    "# for i in date_interval: \n",
    "#     aux = data_cleaning_final('20210414_033707_00348_gn3qg.csv', i, date_interval)\n",
    "#     final = pd.concat([aux, final])\n",
    "# final = final.reset_index(drop = True)  \n",
    "# final.to_csv(\"124125\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chenq\\anaconda3_new\\envs\\tf\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3165: DtypeWarning: Columns (10,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "<ipython-input-2-66b677e2d038>:3: DtypeWarning: Columns (10,16) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  df = data_cleaning_final('20210414_033707_00348_gn3qg.csv','2021-04-07',date_interval)\n",
      "<ipython-input-1-176e339a4fd9>:129: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_date['date'] = date\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('20210414_033707_00348_gn3qg.csv')\n",
    "date_interval = ['2021-04-07','2021-04-08','2021-04-09','2021-04-10','2021-04-11','2021-04-12','2021-04-13']\n",
    "df = data_cleaning_final('20210414_033707_00348_gn3qg.csv','2021-04-07',date_interval)\n",
    "df.to_csv('124125.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#account_id</th>\n",
       "      <th>app_version</th>\n",
       "      <th>ins_date</th>\n",
       "      <th>reward</th>\n",
       "      <th>inter</th>\n",
       "      <th>total</th>\n",
       "      <th>start_date</th>\n",
       "      <th>end_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1616635998640-9042719963125327220</td>\n",
       "      <td>1.2.4</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>2021-04-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1617667022457-168525562445343737</td>\n",
       "      <td>1.2.4</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>2021-04-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1617681886683-3723507210924036161</td>\n",
       "      <td>1.2.4</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>2021-04-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1617720015736-9099722131373122173</td>\n",
       "      <td>1.2.4</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>2021-04-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1617724802200-805334560740813243</td>\n",
       "      <td>1.2.5</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>2021-04-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10369</th>\n",
       "      <td>1617810844031-6991908176145820379</td>\n",
       "      <td>1.2.5</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>2021-04-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10370</th>\n",
       "      <td>1617810867082-7223207295123513573</td>\n",
       "      <td>1.2.4</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>5.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>2021-04-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10371</th>\n",
       "      <td>1617811066333-1342338812460706216</td>\n",
       "      <td>1.2.5</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>2021-04-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10372</th>\n",
       "      <td>1617812674165-7552195953642867155</td>\n",
       "      <td>1.2.5</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>2021-04-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10373</th>\n",
       "      <td>1617818721911-5891064664854436434</td>\n",
       "      <td>1.2.5</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2021-04-07</td>\n",
       "      <td>2021-04-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10374 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             #account_id app_version    ins_date  reward  \\\n",
       "0      1616635998640-9042719963125327220       1.2.4  2021-04-07     0.0   \n",
       "1       1617667022457-168525562445343737       1.2.4  2021-04-07     0.0   \n",
       "2      1617681886683-3723507210924036161       1.2.4  2021-04-07     0.0   \n",
       "3      1617720015736-9099722131373122173       1.2.4  2021-04-07     0.0   \n",
       "4       1617724802200-805334560740813243       1.2.5  2021-04-07     1.0   \n",
       "...                                  ...         ...         ...     ...   \n",
       "10369  1617810844031-6991908176145820379       1.2.5  2021-04-07     0.0   \n",
       "10370  1617810867082-7223207295123513573       1.2.4  2021-04-07     5.0   \n",
       "10371  1617811066333-1342338812460706216       1.2.5  2021-04-07     0.0   \n",
       "10372  1617812674165-7552195953642867155       1.2.5  2021-04-07     0.0   \n",
       "10373  1617818721911-5891064664854436434       1.2.5  2021-04-07     0.0   \n",
       "\n",
       "       inter  total  start_date    end_date  \n",
       "0        0.0    0.0  2021-04-07  2021-04-07  \n",
       "1        0.0    0.0  2021-04-07  2021-04-07  \n",
       "2        0.0    0.0  2021-04-07  2021-04-07  \n",
       "3        0.0    0.0  2021-04-07  2021-04-07  \n",
       "4        3.0    4.0  2021-04-07  2021-04-07  \n",
       "...      ...    ...         ...         ...  \n",
       "10369    3.0    3.0  2021-04-07  2021-04-13  \n",
       "10370   28.0   33.0  2021-04-07  2021-04-13  \n",
       "10371    2.0    2.0  2021-04-07  2021-04-13  \n",
       "10372    6.0    6.0  2021-04-07  2021-04-13  \n",
       "10373    0.0    0.0  2021-04-07  2021-04-13  \n",
       "\n",
       "[10374 rows x 8 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = df[df['end_date'] == '2021-04-07']\n",
    "v124 = test[test['app_version'] == '1.2.4']['total']\n",
    "v125 = test[test['app_version'] == '1.2.5']['total']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-0.7670067089542971, pvalue=0.22159992968823977)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.ttest_ind(v124,v125, alternative = 'less')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('124125.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-07\n",
      "5.349112426035503\n",
      "5.8885400313971745\n",
      "2021-04-08\n",
      "7.549112426035503\n",
      "7.841444270015699\n",
      "2021-04-09\n",
      "8.50059171597633\n",
      "9.10204081632653\n",
      "2021-04-10\n",
      "9.246153846153845\n",
      "10.180533751962324\n",
      "2021-04-11\n",
      "9.53491124260355\n",
      "10.864992150706437\n",
      "2021-04-12\n",
      "9.925443786982248\n",
      "11.232339089481947\n",
      "2021-04-13\n",
      "10.111242603550297\n",
      "11.690737833594977\n"
     ]
    }
   ],
   "source": [
    "date_interval = ['2021-04-07','2021-04-08','2021-04-09','2021-04-10','2021-04-11','2021-04-12','2021-04-13']\n",
    "for i in date_interval:\n",
    "    print(i)\n",
    "    print(df[(df['end_date'] == i) & (df['app_version'] == '1.2.4')]['total'].mean()) \n",
    "    print(df[(df['end_date'] == i) & (df['app_version'] == '1.2.5')]['total'].mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-07\n",
      "2.4615384615384617\n",
      "2.5494505494505493\n",
      "2021-04-08\n",
      "3.6828402366863906\n",
      "3.6671899529042387\n",
      "2021-04-09\n",
      "4.106508875739645\n",
      "4.472527472527473\n",
      "2021-04-10\n",
      "4.502958579881657\n",
      "5.075353218210361\n",
      "2021-04-11\n",
      "4.621301775147929\n",
      "5.489795918367347\n",
      "2021-04-12\n",
      "4.772781065088758\n",
      "5.665620094191523\n",
      "2021-04-13\n",
      "4.861538461538461\n",
      "5.863422291993721\n"
     ]
    }
   ],
   "source": [
    "date_interval = ['2021-04-07','2021-04-08','2021-04-09','2021-04-10','2021-04-11','2021-04-12','2021-04-13']\n",
    "for i in date_interval:\n",
    "    print(i)\n",
    "    print(df[(df['end_date'] == i) & (df['app_version'] == '1.2.4')]['reward'].mean()) \n",
    "    print(df[(df['end_date'] == i) & (df['app_version'] == '1.2.5')]['reward'].mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-07\n",
      "2.8875739644970415\n",
      "3.339089481946625\n",
      "2021-04-08\n",
      "3.8662721893491123\n",
      "4.17425431711146\n",
      "2021-04-09\n",
      "4.3940828402366865\n",
      "4.629513343799058\n",
      "2021-04-10\n",
      "4.743195266272189\n",
      "5.1051805337519625\n",
      "2021-04-11\n",
      "4.913609467455621\n",
      "5.375196232339089\n",
      "2021-04-12\n",
      "5.152662721893491\n",
      "5.566718995290424\n",
      "2021-04-13\n",
      "5.249704142011835\n",
      "5.827315541601256\n"
     ]
    }
   ],
   "source": [
    "date_interval = ['2021-04-07','2021-04-08','2021-04-09','2021-04-10','2021-04-11','2021-04-12','2021-04-13']\n",
    "for i in date_interval:\n",
    "    print(i)\n",
    "    print(df[(df['end_date'] == i) & (df['app_version'] == '1.2.4')]['inter'].mean()) \n",
    "    print(df[(df['end_date'] == i) & (df['app_version'] == '1.2.5')]['inter'].mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-04-08\n",
      "5.257178526841448\n",
      "5.0566318926974665\n",
      "2021-04-09\n",
      "6.6729088639200995\n",
      "6.906110283159464\n",
      "2021-04-10\n",
      "7.298377028714107\n",
      "7.786885245901639\n",
      "2021-04-11\n",
      "7.742821473158552\n",
      "8.72876304023845\n",
      "2021-04-12\n",
      "8.067415730337078\n",
      "9.594634873323399\n",
      "2021-04-13\n",
      "8.540574282147317\n",
      "10.070044709388972\n"
     ]
    }
   ],
   "source": [
    "date_interval = ['2021-04-08','2021-04-09','2021-04-10','2021-04-11','2021-04-12','2021-04-13']\n",
    "for i in date_interval:\n",
    "    print(i)\n",
    "    print(df[(df['end_date'] == i) & (df['app_version'] == '1.2.4')]['total'].mean()) \n",
    "    print(df[(df['end_date'] == i) & (df['app_version'] == '1.2.5')]['total'].mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3112.0\n",
      "3267.0\n",
      "2336.0\n",
      "2659.0\n"
     ]
    }
   ],
   "source": [
    "print(df[(df['end_date'] == '2021-04-08') & (df['app_version'] == '1.2.4')]['reward'].sum())\n",
    "print(df[(df['end_date'] == '2021-04-08') & (df['app_version'] == '1.2.4')]['inter'].sum()) \n",
    "\n",
    "print(df[(df['end_date'] == '2021-04-08') & (df['app_version'] == '1.2.5')]['reward'].sum())\n",
    "print(df[(df['end_date'] == '2021-04-08') & (df['app_version'] == '1.2.5')]['inter'].sum()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t124 = df[(df['end_date'] == '2021-04-07') & (df['app_version'] == '1.2.4')]['total'].mean()\n",
    "t125 = df[(df['end_date'] == '2021-04-07') & (df['app_version'] == '1.2.5')]['total'].mean()\n",
    "stats.ttest_ind(t124, t125)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power of function for non-parametric test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_interval = check_factor_level('$part_date',df)\n",
    "final = data_cleaning_final('20210414_033707_00348_gn3qg.csv', date_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#account_id</th>\n",
       "      <th>app_version</th>\n",
       "      <th>ins_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1616310537153-4096451041333916560</td>\n",
       "      <td>1.2.4</td>\n",
       "      <td>2021-04-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1616595508775-1436936675628725227</td>\n",
       "      <td>1.2.4</td>\n",
       "      <td>2021-04-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1616632939573-937128715024629693</td>\n",
       "      <td>1.2.4</td>\n",
       "      <td>2021-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1616635998640-9042719963125327220</td>\n",
       "      <td>1.2.4</td>\n",
       "      <td>2021-04-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1616774716492-4881919259140384884</td>\n",
       "      <td>1.2.5</td>\n",
       "      <td>2021-04-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9779</th>\n",
       "      <td>1618329335521-7143476158041775056</td>\n",
       "      <td>1.2.5</td>\n",
       "      <td>2021-04-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9780</th>\n",
       "      <td>1618329376623-5465467047830775450</td>\n",
       "      <td>1.2.4</td>\n",
       "      <td>2021-04-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9781</th>\n",
       "      <td>1618329418719-3648959904582759357</td>\n",
       "      <td>1.2.5</td>\n",
       "      <td>2021-04-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9782</th>\n",
       "      <td>1618329445540-2754650690079065775</td>\n",
       "      <td>1.2.5</td>\n",
       "      <td>2021-04-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9783</th>\n",
       "      <td>1618329561690-7494415202455834565</td>\n",
       "      <td>1.2.5</td>\n",
       "      <td>2021-04-13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9784 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            #account_id app_version    ins_date\n",
       "0     1616310537153-4096451041333916560       1.2.4  2021-04-13\n",
       "1     1616595508775-1436936675628725227       1.2.4  2021-04-11\n",
       "2      1616632939573-937128715024629693       1.2.4  2021-04-09\n",
       "3     1616635998640-9042719963125327220       1.2.4  2021-04-07\n",
       "4     1616774716492-4881919259140384884       1.2.5  2021-04-09\n",
       "...                                 ...         ...         ...\n",
       "9779  1618329335521-7143476158041775056       1.2.5  2021-04-13\n",
       "9780  1618329376623-5465467047830775450       1.2.4  2021-04-13\n",
       "9781  1618329418719-3648959904582759357       1.2.5  2021-04-13\n",
       "9782  1618329445540-2754650690079065775       1.2.5  2021-04-13\n",
       "9783  1618329561690-7494415202455834565       1.2.5  2021-04-13\n",
       "\n",
       "[9784 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['count'] = 1\n",
    "aux = df[df['#event_name'] == 'install'].groupby(['#account_id','$part_date', 'app_version'])['count'].sum()\n",
    "df = list_to_frame(aux)\n",
    "df['ins_date'] = df['$part_date']\n",
    "df_ins = df.drop(['$part_date', 'count'], axis = 1)\n",
    "df_ins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpe = trimmed_sample_left('reward',df, 0.1)\n",
    "helpe.to_csv(\"113114_left.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpe = winsorized_sample('reward',df, 0.1)\n",
    "helpe.to_csv(\"113114_winsorized.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimental design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-way ANOVA with fixed model (balanced data):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idea about variability(no variance): \n",
    "$$\\sum_{i=1}^{N} (x_i - \\overline{x})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(y_1, x_1), (y_2,x_2)...(y_n, x_n)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one fixed effect model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}\n",
    "\\begin{cases}\n",
    "i = 1, ..., a = 2 \\\\\n",
    "j = 1, 2, ...,n\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$ \n",
    "\\sum_{i=1}^{2} \\alpha_i = 0\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\epsilon_{ij} \\sim {\\sf N}(0, \\sigma^2)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Statistics: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "SS_{T} = \\sum_{i=1}^{2} \\sum_{j=1}^{n} (y_{ij} - \\overline{y}_{..})^2\n",
    "\\\\\n",
    "SS_{TR} = \\sum_{i=1}^{2} \\sum_{j=1}^{n} (\\overline{y}_{i.} - \\overline{y}_{..})^2\n",
    "\\\\\n",
    "SS_{E} = \\sum_{i=1}^{2} \\sum_{j=1}^{n} (y_{ij} - \\overline{y}_{i.})^2\n",
    "\\\\\n",
    "SS_{T} = SS_{TR} + SS_{E}\n",
    "\\\\\n",
    "R^2 = \\frac{SS_{TR}}{SS_{T}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Degrees of freedom:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "DF(SS_T) = N - 1, N = 2*n \\\\\n",
    "\\\\\n",
    "DF(SS_{TR}) = a - 1 = 2 - 1 = 1 \\\\ \n",
    "\\\\\n",
    "DF(SS_{E}) = N - a = 2n - 2 \\\\\n",
    "\\\\\n",
    "MS_{TR} = \\frac{SS_{TR}}{a - 1} = SS_{TR} \\\\\n",
    "\\\\\n",
    "MS_{E} = \\frac{SS_E}{N - a} = \\frac{SS_E}{2n - 2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hypotesis:\n",
    "$$ H_0: \\alpha_1 = \\alpha_2 \\\\\n",
    "   H_1: \\alpha_1 \\not= \\alpha_2 \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\alpha = P(type$ $I$ $error) = P(reject H_0|H_0$ $is$ $true)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\beta = P(type$ $II$ $error) = P(fail$ $to$ $reject$ $H_0|H_0$ $is$ $false)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Power = 1 -\\beta = P(reject H_0|H_0$ $is$ $false)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{SS_E}{\\sigma^2} \\sim \\chi^2, (Fisher) \\\\\n",
    "\\\\\n",
    "E(MS_E) = \\sigma^2, E(\\frac{SS_E}{\\sigma^2}) = N - a \\\\\n",
    "\\\\\n",
    "SS_{TR} \\sim \\chi^2_{1}\\mid H_0 \\\\\n",
    "\\\\\n",
    "E(MS_{TR}) = \\sigma^2 + n\\frac{\\sum_{i = 1}^{2} \\alpha^2_{i}}{2 - 1}, (Cochran) \\\\\n",
    "\\\\\n",
    "\\frac{MS_{TR}}{MS_{E}} \\sim F_{2-1, 2n - 2} \\\\\n",
    "\\\\\n",
    "CI: \\left\\{ \\frac{MS_{TR}}{MS_E} > F_{2-1, 2n - 2,\\alpha} \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\hat{\\mu} = \\overline{y}_{..} \\\\\n",
    "\\\\\n",
    "\\hat{\\alpha}_i = \\overline{y}_{i.} - \\overline{y}_{..} \\\\ \n",
    "\\\\\n",
    "\\hat{\\mu + \\alpha_i} = \\overline{y}_{i.} \\\\\n",
    "\\\\\n",
    "\\hat{y}_{ij} = \\hat{\\mu} + \\hat{\\alpha}_{i} \\\\\n",
    "\\\\\n",
    "\\epsilon_{ij} = y_{ij} - \\hat{y}_{ij} = y_{ij} - \\hat{\\mu} - \\hat{\\alpha}_i \\\\\n",
    "\\\\\n",
    "SS_{TR} = \\sum_{i=1}^{a} \\sum_{j=1}^{n} \\hat{\\alpha}_i^2 = n\\left( \\hat{\\alpha}_1^2+\\hat{\\alpha}_2^2+...+\\hat{\\alpha}_a^2 \\right) \\\\\n",
    "\\\\\n",
    "SS_E = \\sum_{i=1}^{a} \\sum_{j=1}^{n} (y_{ij} - \\hat{y}_{ij})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can get: \n",
    "$$ \n",
    "y_{11},..., y_{1n} \\sim N(\\mu + \\alpha_1, \\sigma^2) \\\\\n",
    "\\\\\n",
    "y_{21},..., y_{2n} \\sim N(\\mu + \\alpha_2, \\sigma^2) \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbiased estimate(mean, variance):\n",
    "$$\\overline{x} = \\frac{1}{n} \\sum_{i=1}{n} x_i \\\\\n",
    "s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\overline{x})^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### one way ANOVA： \n",
    "\n",
    " VS   | SS   | DF     | MS   |     F    |\n",
    "----- | ---- |------- |------|----------|\n",
    " TR   | SSTR | a - 1  | MSTR | MSTR/MSE |\n",
    "Error | SSE  | N - a  | MSE  |          |   \n",
    "Total | SST  | N - 1  |      |          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unbalanced data:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ y_{ij} = \\mu + \\alpha_i + \\epsilon_{ij}\n",
    "\\begin{cases}\n",
    "i = 1, ..., a = 2 \\\\\n",
    "j = 1, 2, ...,n_i\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$\n",
    "N = \\sum_{i=1}^{a} n_i \\\\\n",
    "\\\\\n",
    "\\sum_{i=1}^{2} \\alpha_i = 0 \\\\\n",
    "\\\\\n",
    "\\epsilon_{ij} \\sim {\\sf N}(0, \\sigma^2) \\\\\n",
    "\\\\\n",
    "SS_{T} = \\sum_{i=1}^{2} \\sum_{j=1}^{n_i} (y_{ij} - \\overline{y}_{..})^2 \\\\\n",
    "\\\\\n",
    "SS_{TR} = \\sum_{i=1}^{2} \\sum_{j=1}^{n_i} (\\overline{y}_{i.} - \\overline{y}_{..})^2 \\\\\n",
    "\\\\\n",
    "SS_{E} = \\sum_{i=1}^{2} \\sum_{j=1}^{n_i} (y_{ij} - \\overline{y}_{i.})^2\n",
    "$$\n",
    "\n",
    "### There are two advantages in choosing a balanced design. First, the test statistic is relatively insensitive to small departures from the assumption of equal variances for the a treatments if the sample sizes are equal. This is not the case for unequal sample sizes. Second, the power of the test is maximized if the samples are of equal size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation: analysis of residual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Normality: qq-plot, Kolmogorov-Smirnov test, shapiro-wilk test\n",
    "### 2. Independent: Durbin-watson statistic \n",
    "$$\n",
    "d = \\frac{\\sum_{t=2}^{T}(e_t - e_{t-1})^2}{\\sum_{t=1}^{T} e_t^2}\n",
    "$$\n",
    "\n",
    "\n",
    "### T is the number of observations, since d is approximately equal to $2(1 - \\hat{\\rho})$, where $\\hat{\\rho}$, in general, \n",
    "\n",
    "$$\n",
    "\\rho = \\frac{cov(x, y)}{\\sqrt{var(x)}\\sqrt{var(y)}}\n",
    "$$\n",
    "\n",
    "###  $d = 2$ indicates no autocorrelation, The value of $d$ always lies between $0$ and $4$. If the Durbin–Watson statistic is substantially less than $2$, there is evidence of positive serial correlation. As a rough rule of thumb, if Durbin–Watson is less than $1.0$, there may be cause for alarm. Small values of $d$ indicate successive error terms are positively correlated. If $d > 2$, successive error terms are negatively correlated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Homoscedasticity: Levene test \n",
    "$$\n",
    "H_0: \\sigma_1 = ... = \\sigma_a\n",
    "\\\\\n",
    "H_1: \\exists i \\not= j, \\sigma_i \\not= \\sigma_j\n",
    "$$\n",
    "\n",
    "$D_{ij} = |e_{ij}|$\n",
    "\n",
    "$\\overline{D}_{i.} = \\sigma_i$\n",
    "### Apply experiment design. \n",
    "$$\n",
    "CI: \\left\\{ \\frac{MS_{TR}(D)}{MS_E(D)} > F_{a-1, N - 2,\\alpha} \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practically, log transformation for 1 and 3. In general, Using box-cox transformation. \n",
    "\n",
    "### Box-Cox transformation \n",
    "### $I$ groups with observations, $i = 1,2,...,I$, $\\mu_i$ and $\\sigma_i^2$ for each group. \n",
    "$$ \n",
    "\\exists c,\\alpha \\in \\mathbb{R}|\\sigma_i = c\\mu_i^\\alpha \\\\\n",
    "\\\\\n",
    "\\lambda = 1 - \\alpha\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In practice, maximum likelihood method to estimate $c$ and $\\alpha$\n",
    "$$\n",
    "L(\\lambda;y^{\\lambda}_1,...,y^{\\lambda}_n = -\\frac{n}{2}lnSSE(\\lambda)\n",
    "$$\n",
    "\n",
    "### Formula for transformation:\n",
    "$$\n",
    "y^{\\lambda} = \n",
    "\\begin{cases}\n",
    "\\frac{y^{\\lambda} - 1}{\\lambda\\tilde{y}^{\\lambda -1}}, \\lambda \\not= 0 \\\\\n",
    "\\tilde{y}lny, \\lambda = 0 \n",
    "\\end{cases}\n",
    "$$\n",
    "### $\\tilde{y} = \\sqrt[n]{y_i***y_n}$(geometric mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normality can not handle, using nonparametric test. \n",
    "\n",
    "## 1 quantitative + 1 cualitative\n",
    "### 2 samples dependent:  wilcoxon test\n",
    "### 2 samples independent : wilcoxon mann whitney test \n",
    "### k samples dependent: Friedman test \n",
    "### k samples independent: Kruskal wallis test \n",
    "## 2 cualitative \n",
    "### dependent: McNemar test \n",
    "### independent: chi-squared test \n",
    "## 2 quantitative\n",
    "### kendall test, Spearman test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wilcoxon mann whitney test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "H_0: F_x = F_y \\\\\n",
    "H_1: F_x \\not= F_y\\\\\n",
    "\\\\\n",
    "(x_1,...,x_n), (y_1,...,y_m)\\\\\n",
    "n < m \\\\\n",
    "\\Psi_{ij} = \\begin{cases}\n",
    "1, y_j > x_i \\\\\n",
    "0, y_j < x_i \\\\\n",
    "\\end{cases}\n",
    "i = 1...n, j=1...m \\\\\n",
    "U_Y = \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\Psi_{ij} \\\\\n",
    "\\forall y_j > x_i, U_Y = nm\\\\\n",
    "\\forall y_j < x_i, U_Y = 0\\\\ \n",
    "U_Y = [0, nm] \\\\\n",
    "\\Psi_{ij}^* = \\begin{cases}\n",
    "1, y_j < x_i \\\\\n",
    "0, y_j > x_i \\\\\n",
    "\\end{cases}\n",
    "i = 1...n, j=1...m \\\\\n",
    "U_X = \\sum_{i=1}^{n} \\sum_{j=1}^{m} \\Psi_{ij}^* \\\\\n",
    "U_X = nm - U_Y\\\\\n",
    "x_i = y_i, \\Psi_{ij} = \\frac{1}{2}, \\Psi_{ij}^* = \\frac{1}{2}\n",
    "U = min(U_X, U_Y)\\\\\n",
    "n,m < 30, table\\\\\n",
    "n, m > 30, U \\sim N\\left( \\frac{nm}{2}, \\sqrt{\\frac{nm(n+m+1)}{12}} \\right) \\\\\n",
    "CI: \\left\\{ U \\le k_1 \\right\\} \\cup \\left\\{ U \\ge k_2 \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific comparision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Least significant difference (Fisher)\n",
    "$$\n",
    "H_0: \\mu + \\alpha_i = \\mu + \\alpha_j \\forall i \\not= j \\\\\n",
    "H_1: \\mu + \\alpha_i \\not= \\mu + \\alpha_j\n",
    "\\\\\n",
    "CI(LSD):|\\overline{y_{i.}}-\\overline{y_{j.}}| > t_{N-a, \\frac{\\alpha}{2}}\\sqrt{MS_E\\left(\\frac{1}{n}+\\frac{1}{n}\\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonferonni\n",
    "$$\n",
    "H_0: \\mu + \\alpha_i = \\mu + \\alpha_j \\forall i \\not= j \\\\ \n",
    "H_1: \\mu + \\alpha_i \\not= \\mu + \\alpha_j\n",
    "\\\\\n",
    "CI(LSD):|\\overline{y_{i.}}-\\overline{y_{j.}}| > t_{N-a, \\frac{\\alpha_{\\beta}}{2}}\\sqrt{MS_E\\left(\\frac{1}{n}+\\frac{1}{n}\\right)}\\\\\n",
    "\\alpha_{\\beta} = \\frac{\\alpha}{\\binom{a}{2}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Honestly significant difference (Tukey)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "(x_1,...,x_n) \\sim N(0, \\sigma^2) \\\\\n",
    "Studentized : \\frac{X}{\\hat{\\sigma}} \\\\\n",
    "q(l,n) = \\frac{max{\\{x_1,..,x_n\\}} - min{\\{x_1,...,x_n\\}}}{\\hat{\\sigma}} \\\\\n",
    "\\frac{l\\hat{\\sigma}^2}{\\sigma^2} \\sim \\chi_l^2 \\\\\n",
    "\\overline{y}_{1.} \\le \\overline{y}_{2.} \\le...\\overline{y}_{a.}\\\\\n",
    "H_0: \\mu + \\alpha_1 = \\mu + \\alpha_a \\\\ \n",
    "H_1: \\mu + \\alpha_1 \\not= \\mu + \\alpha_a\\\\\n",
    "\\frac{\\overline{y}_{a.}-\\overline{y}_{1.}}{\\sqrt{\\frac{MS_E}{n}}}|H_0 \\sim q_\\alpha(N - a, a) \\\\\n",
    "CI(HSD): \\overline{y}_{a.}-\\overline{y}_{1.} > q_\\alpha(N - a, a)\\sqrt{\\frac{MS_E}{n}} \\\\\n",
    "H_0: \\mu + \\alpha_i = \\mu + \\alpha_j  \\\\ \n",
    "H_1: \\mu + \\alpha_i \\not= \\mu + \\alpha_j \\\\\n",
    "CI(HSD): |\\overline{y}_{i.}-\\overline{y}_{j.}| > q_{\\alpha}(N - a, a)\\sqrt{\\frac{MS_E}{n}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Newman-Keuls Method \n",
    "$$\n",
    "H_0: \\mu + \\alpha_i = \\mu + \\alpha_j  \\\\ \n",
    "H_1: \\mu + \\alpha_i \\not= \\mu + \\alpha_j \\\\\n",
    "CI = \\left\\{ |\\overline{y}_{i.}-\\overline{y}_{j.}| > q_\\alpha(N-a,|pos_{(i)} - pos_{(j)} + 1|)\\sqrt{\\frac{MS_E}{n}} \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Duncan Method \n",
    "$$\n",
    "H_0: \\mu + \\alpha_i = \\mu + \\alpha_j  \\\\ \n",
    "H_1: \\mu + \\alpha_i \\not= \\mu + \\alpha_j \\\\\n",
    "CI = \\left\\{ |\\overline{y}_{i.}-\\overline{y}_{j.}| > r_\\alpha(N-a,|pos_{(i)} - pos_{(j)} + 1|)\\sqrt{\\frac{MS_E}{n}} \\right\\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dunnet method \n",
    "\n",
    "$$\n",
    "H_0: \\mu + \\alpha_i = \\mu + \\alpha_a  \\\\ \n",
    "H_1: \\mu + \\alpha_i \\not= \\mu + \\alpha_a \\\\\n",
    "CI = \\left\\{ |\\overline{y}_{i.}-\\overline{y}_{a.}| > d_\\alpha(N-a)\\sqrt{MS_E\\left(\\frac{1}{n}+\\frac{1}{n}\\right)} \\right\\}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeated Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Robust inference(One-Way Robust testing) \n",
    "\n",
    "$$\n",
    "H_0: \\mu_{t1} = \\mu_{t2} =...=\\mu_{tJ}  \n",
    "$$\n",
    "### Using trimmed means rather than means (20% trimming level) cuts off 20% at the lower end and 20% the higher end of the distribution\n",
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "d = \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF模型和AB test内容说明\n",
    "\n",
    "## 一份完整的数据挖掘报告分为以下四个部分：\n",
    "## 1. 版本变更内容说明\n",
    "## 2. 数据可视化\n",
    "## 3. 统计建模\n",
    "## &ensp; 3.1 RF模型\n",
    "## &ensp; 3.2 AB test模型\n",
    "## 4. 结论报告\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "### 1. 版本变更说明\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "<font size=4>\n",
    "此部分说明游戏的版本变动内容以及依据这些变动确定参数矩阵(metric，包含直接受影响的参数矩阵 + 激励和插页观看次数矩阵)，同时描述样本的基础信息。 \n",
    "</font>\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "### 2. 数据可视化\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "<font size=4>\n",
    "此部分分为两块，第一块是根据两个版本的游戏的基础数据信息（留存率，广告点击次数之类的重要指标）可视化，此处会用到最为基础的柱状图，饼图，漏斗模型图等。第二块是服务于RF，聚类模型和AB test的可视化，包括RF模型的用户散点图，基于参数矩阵的频率分布柱状图，箱型图，交互作用图等。\n",
    "</font>\n",
    "    \n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "### 3. 统计建模\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "<font size=4>\n",
    "第3部分主要是对所选择的模型的大概描述和介绍，比如，目前做AB test所选择的 ANOVA 和 wilcoxon mann whitney test。给出具体结果table。考虑具体建模过程可以appendix的形式放在最后以供验收和校正使用，该部分主要是用于每次统计模型的判断将和经验主义所做出的判断一起与实际情况进行比较，从而不断修正模型，供数据中台内部使用。   \n",
    "</font>    \n",
    "\n",
    "### 4. 结论报告\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "<font size=4>\n",
    "第四部分是基于第三部分的统计结果做出结论，版本之间是否有显著性差异，直接受影响矩阵是否影响了最终的目标矩阵，比如点击率的升高是否会直接使激励的点击次数提升。  \n",
    "</font>  \n",
    "\n",
    "## 整体分析思路：\n",
    "## RF模型 & 聚类\n",
    "\n",
    "<font size=4>\n",
    "1. RF模型的目的是给所有玩游戏的用户做分类找出“有问题但有救用户”，设为目标用户。该类用户是在游戏时长或者点击次数某一维度存在问题的用户，找出该部分用户是进一步做归因分析的前提，归因分析是后续结合经验调整版本的依据。用户在游戏时长或者点击次数都很低的不会被作为目标用户是因为该类用户在游戏中的行为过少无法分析，这部分用户只能凭借经验主义去优化。同时，游戏时长和点击次数都高的“优秀用户”也会被监控，确保版本的变化不会影响该部分用户。</font>\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "\n",
    "<font size=4>\n",
    "2. 通过RF模型的分类找出目标群体后，使用K-means方法或clustering方法给目标群体进行聚类分群，使用不同的指标一起作为分类标准，最终把目标群体按照他们游戏内的不同行为分成几组，而后通过每组内群体的游戏行为的不同推出此类用户游戏时长或点击次数不足的原因。  \n",
    "</font>\n",
    "\n",
    "### 再结合AB版本二次聚类，找出A，B版本中靠近的群设定为AA test的sample \n",
    "\n",
    "## 版本比较 ( AB test ) \n",
    "\n",
    "<font size=4>\n",
    "1. 版本变动后分别在A，B两个版本做AA test，使用2中的sample，此处使用repeated measure model， 理论上会获得两个结论，A版本是否有显著性差异和B版本是否有显著性差异。理想情况是，A版本并无明显显著性差异，B版本有显著性差异。 \n",
    "</font>\n",
    "\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "<font size=4>\n",
    "2. 根据版本的变动所造成的影响选择复合或单个metric做AB test，此处为A，B版本间比较，使用robust ANOVA 和 non parametric test，按不同天数得到结论，理想情况是，A和B版本在robust ANOVA和non parametric上存在显著性差异。 \n",
    "</font>\n",
    "\n",
    "$$\n",
    "\\\\\n",
    "$$\n",
    "<font size=4>\n",
    "3. 优化：缩短test时间周期，增加factor，增加factor的dimension，glm，ANCOVA,  time series model( autocorrelation ) 看是否可以用上。 \n",
    "</font>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
